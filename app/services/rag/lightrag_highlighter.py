"""
LightRAG Highlighter - Skeleton Strategy
========================================
Module n√†y gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ highlight cho LightRAG khi:
1. Text b·ªã l·ªói kho·∫£ng tr·∫Øng (ti ·ªÅn -> ti·ªÅn).
2. Chunk qu√° d√†i, tr·∫£i qua nhi·ªÅu trang.
3. Chunk ch·ª©a header/footer l·∫∑p l·∫°i.

Gi·∫£i thu·∫≠t:
1. Load to√†n b·ªô PDF th√†nh m·ªôt d√≤ng ch·∫£y c√°c t·ª´ (Global Word Stream).
2. T·∫°o "Skeleton String" b·∫±ng c√°ch n·ªëi c√°c t·ª´ l·∫°i v√† b·ªè qua kho·∫£ng tr·∫Øng/d·∫•u c√¢u th·ª´a.
3. D√πng SequenceMatcher t√¨m ƒëo·∫°n kh·ªõp d√†i nh·∫•t tr√™n Skeleton.
4. Map ng∆∞·ª£c t·ª´ k√Ω t·ª± trong Skeleton ra t·ªça ƒë·ªô (BBox) c·ªßa t·ª´ g·ªëc.
"""

import re
import logging
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

@dataclass
class HighlightArea:
    """ƒê·∫°i di·ªán cho m·ªôt v√πng highlight (t·ªça ƒë·ªô %)"""
    pageIndex: int
    left: float
    top: float
    width: float
    height: float

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class ChunkHighlight:
    """Ch·ª©a danh s√°ch highlight cho m·ªôt chunk"""
    chunkId: str
    pageIndex: int
    text: str
    areas: List[HighlightArea]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "chunkId": self.chunkId,
            "pageIndex": self.pageIndex,
            "text": self.text[:100] + "..." if len(self.text) > 100 else self.text,
            "areas": [area.to_dict() for area in self.areas]
        }

class LightRAGHighlighter:
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.doc: Optional[fitz.Document] = None
        
        # Mapping quan tr·ªçng:
        # self.global_words: List c√°c t·ª´ trong to√†n b·ªô PDF (k√®m trang, bbox)
        self.global_words: List[Dict[str, Any]] = []
        
        # self.doc_skeleton: Chu·ªói string kh·ªïng l·ªì ƒë·∫°i di·ªán cho c·∫£ PDF (ƒë√£ clean, vi·∫øt li·ªÅn kh√¥ng d·∫•u c√°ch)
        self.doc_skeleton: str = ""
        
        # self.char_to_word_idx: Map index k√Ω t·ª± trong skeleton -> index t·ª´ trong global_words
        # VD: k√Ω t·ª± th·ª© 100 trong skeleton thu·ªôc v·ªÅ t·ª´ th·ª© 20 trong global_words
        self.char_to_word_idx: List[int] = []
        
        self._load_pdf()

    def _load_pdf(self):
        """Load PDF v√† x√¢y d·ª±ng Skeleton Index"""
        try:
            self.doc = fitz.open(self.pdf_path)
            self.global_words = []
            self.doc_skeleton = ""
            self.char_to_word_idx = []

            for page_idx, page in enumerate(self.doc):
                # get_text("words") tr·∫£ v·ªÅ: (x0, y0, x1, y1, "text", block_no, line_no, word_no)
                words = page.get_text("words")
                
                for w in words:
                    text_content = w[4]
                    
                    # T·∫°o skeleton cho t·ª´ n√†y (b·ªè d·∫•u c√°ch, lower case)
                    clean_text = self._make_skeleton_segment(text_content)
                    
                    if not clean_text:
                        continue

                    # 1. L∆∞u th√¥ng tin t·ª´ v√†o danh s√°ch to√†n c·ª•c
                    self.global_words.append({
                        "page": page_idx,
                        "bbox": w[:4], # (x0, y0, x1, y1)
                        "text": text_content
                    })
                    
                    # 2. C·∫≠p nh·∫≠t chu·ªói skeleton
                    self.doc_skeleton += clean_text
                    
                    # 3. Map t·ª´ng k√Ω t·ª± c·ªßa t·ª´ n√†y v·ªÅ index c·ªßa t·ª´ trong list global_words
                    word_idx = len(self.global_words) - 1
                    self.char_to_word_idx.extend([word_idx] * len(clean_text))

            logger.info(f"üìñ Indexed PDF: {len(self.global_words)} words, skeleton len: {len(self.doc_skeleton)}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load PDF {self.pdf_path}: {e}")
            raise

    def _make_skeleton_segment(self, text: str) -> str:
        """
        Bi·∫øn text th√†nh d·∫°ng x∆∞∆°ng s·ªëng: lower, b·ªè space, b·ªè d·∫•u c√¢u nhi·ªÖu.
        Gi·ªØ l·∫°i ch·ªØ c√°i v√† s·ªë, bao g·ªìm ti·∫øng Vi·ªát.
        """
        text = text.lower()
        # X√≥a to√†n b·ªô whitespace, newline, tab
        text = re.sub(r'\s+', '', text)
        return text

    def find_all_highlights(self, chunks: List[Dict[str, Any]]) -> List[ChunkHighlight]:
        """T√¨m highlight cho danh s√°ch chunks"""
        results = []
        if not self.doc: return results

        for chunk in chunks:
            chunk_id = chunk.get("chunk_id", chunk.get("id", "unknown"))
            raw_content = chunk.get("content", chunk.get("text", ""))
            
            if not raw_content: continue

            # 1. T·∫°o skeleton cho chunk (chunk_skeleton c≈©ng s·∫Ω kh√¥ng c√≥ d·∫•u c√°ch)
            chunk_skeleton = self._make_skeleton_segment(raw_content)
            
            # B·ªè qua n·∫øu chunk qu√° ng·∫Øn (d∆∞·ªõi 10 k√Ω t·ª± th·ª±c)
            if len(chunk_skeleton) < 10: 
                continue 

            # 2. T√¨m ki·∫øm chu·ªói chunk trong chu·ªói PDF (Fuzzy Match)
            # SequenceMatcher t√¨m ƒëo·∫°n tr√πng d√†i nh·∫•t (longest contiguous matching block)
            # autojunk=False ƒë·ªÉ tr√°nh n√≥ b·ªè qua c√°c t·ª´ l·∫∑p l·∫°i nhi·ªÅu
            matcher = SequenceMatcher(None, self.doc_skeleton, chunk_skeleton, autojunk=False)
            
            # T√¨m ƒëo·∫°n kh·ªõp d√†i nh·∫•t
            match = matcher.find_longest_match(0, len(self.doc_skeleton), 0, len(chunk_skeleton))

            # Ng∆∞·ª°ng ch·∫•p nh·∫≠n: ƒêo·∫°n kh·ªõp ph·∫£i c√≥ ƒë·ªô d√†i t∆∞∆°ng ƒë·ªëi
            if match.size < 15: # Qu√° ng·∫Øn (< 15 k√Ω t·ª± li·ªÅn m·∫°ch) th√¨ coi nh∆∞ kh√¥ng t√¨m th·∫•y
                logger.warning(f"‚ö†Ô∏è Chunk {chunk_id}: No good match found (size={match.size})")
                continue

            # 3. Map t·ª´ v·ªã tr√≠ k√Ω t·ª± (Skeleton) -> Danh s√°ch t·ª´ (Words)
            doc_start_char = match.a
            doc_end_char = match.a + match.size
            
            try:
                # Map ng∆∞·ª£c t·ª´ k√Ω t·ª± sang index c·ªßa t·ª´ trong global_words
                start_word_idx = self.char_to_word_idx[doc_start_char]
                # doc_end_char l√† exclusive, n√™n l·∫•y char li·ªÅn tr∆∞·ªõc ƒë√≥
                end_word_idx = self.char_to_word_idx[doc_end_char - 1]
            except IndexError:
                # Tr∆∞·ªùng h·ª£p boundary edge case
                continue

            # L·∫•y danh s√°ch c√°c t·ª´ ƒë√£ match
            # +1 ·ªü end_word_idx v√¨ slicing trong python l√† exclusive
            matched_global_words = self.global_words[start_word_idx : end_word_idx + 1]
            
            if not matched_global_words:
                continue

            # 4. Convert c√°c t·ª´ th√†nh HighlightArea (t·ªça ƒë·ªô %)
            areas = self._words_to_areas(matched_global_words)
            
            # 5. G·ªôp c√°c t·ª´ th√†nh d√≤ng (Merge)
            merged_areas = self._merge_areas(areas)

            # 6. X√°c ƒë·ªãnh trang ch√≠nh (trang ch·ª©a nhi·ªÅu highlight nh·∫•t)
            page_counts = {}
            for a in merged_areas:
                page_counts[a.pageIndex] = page_counts.get(a.pageIndex, 0) + 1
            primary_page = max(page_counts, key=page_counts.get) if page_counts else 0

            results.append(ChunkHighlight(
                chunkId=chunk_id,
                pageIndex=primary_page,
                text=raw_content,
                areas=merged_areas
            ))
            
            logger.info(f"‚úÖ Chunk {chunk_id}: Match found on Page {primary_page + 1} (Words: {len(matched_global_words)})")

        return results

    def _words_to_areas(self, words: List[Dict[str, Any]]) -> List[HighlightArea]:
        """Chuy·ªÉn ƒë·ªïi danh s√°ch t·ª´ (bbox tuy·ªát ƒë·ªëi) sang HighlightArea (bbox %)"""
        areas = []
        for w in words:
            page_idx = w['page']
            page_obj = self.doc[page_idx]
            pw = page_obj.rect.width
            ph = page_obj.rect.height
            
            x0, y0, x1, y1 = w['bbox']
            
            areas.append(HighlightArea(
                pageIndex=page_idx,
                left=(x0 / pw) * 100,
                top=(y0 / ph) * 100,
                width=((x1 - x0) / pw) * 100,
                height=((y1 - y0) / ph) * 100
            ))
        return areas

    def _merge_areas(self, areas: List[HighlightArea]) -> List[HighlightArea]:
        """G·ªôp c√°c highlight t·ª´ng t·ª´ th√†nh d√≤ng ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n"""
        if not areas: return []
        
        # S·∫Øp x·∫øp: Trang -> D√≤ng (Top) -> Tr√°i (Left)
        # L√†m tr√≤n top ƒë·ªÉ c√°c t·ª´ c√πng d√≤ng nh∆∞ng l·ªách pixel v·∫´n g·ªôp ƒë∆∞·ª£c
        areas.sort(key=lambda x: (x.pageIndex, round(x.top, 1), x.left))
        
        merged = []
        current = areas[0]
        
        for next_area in areas[1:]:
            # 1. Kh√°c trang -> ng·∫Øt
            if current.pageIndex != next_area.pageIndex:
                merged.append(current)
                current = next_area
                continue
            
            # 2. Kh√°c d√≤ng (ƒë·ªô l·ªách d·ªçc > 1.5%) -> ng·∫Øt
            if abs(current.top - next_area.top) > 1.5:
                merged.append(current)
                current = next_area
                continue
            
            # 3. Qu√° xa nhau theo chi·ªÅu ngang (> 20%) -> ng·∫Øt
            # Cho ph√©p kho·∫£ng c√°ch l·ªõn (20%) ƒë·ªÉ n·ªëi ƒë∆∞·ª£c c√°c t·ª´ b·ªã r√°ch r·ªùi nh∆∞ "ti ·ªÅn"
            horizontal_gap = next_area.left - (current.left + current.width)
            if horizontal_gap > 20.0: 
                merged.append(current)
                current = next_area
                continue
                
            # G·ªòP: M·ªü r·ªông current area bao tr√πm next_area
            new_left = min(current.left, next_area.left)
            new_top = min(current.top, next_area.top)
            # Right edge m·ªõi = max c·ªßa (left + width)
            current_right = current.left + current.width
            next_right = next_area.left + next_area.width
            new_width = max(current_right, next_right) - new_left
            new_height = max(current.height, next_area.height)
            
            current = HighlightArea(
                pageIndex=current.pageIndex,
                left=new_left,
                top=new_top,
                width=new_width,
                height=new_height
            )
            
        merged.append(current)
        return merged

    def close(self):
        if self.doc: self.doc.close()

    def __enter__(self): return self
    def __exit__(self, exc_type, exc_val, exc_tb): self.close()