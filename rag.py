"""
FastAPI RAG System - PhiÃªn báº£n Ä‘Æ¡n giáº£n dá»… hiá»ƒu vá»›i ChromaDB
YÃªu cáº§u cÃ i Ä‘áº·t:
pip install fastapi uvicorn python-multipart langchain langchain-openai langchain-google-genai langchain-chroma chromadb pypdf python-docx
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import os
import tempfile
from pathlib import Path
import uuid
import shutil
from app.core.config import settings

# Imports cÆ¡ báº£n
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI

# Khá»Ÿi táº¡o FastAPI app
app = FastAPI(title="RAG System API - Simple Version")

# Biáº¿n toÃ n cá»¥c lÆ°u vector store
vector_store = None
embeddings = None
llm = None

# Cáº¥u hÃ¬nh API keys
GOOGLE_API_KEY = settings.GEMINI_API_KEY
LLM_BASE_URL = settings.LLM_BASE_URL

# ChromaDB config
CHROMA_PERSIST_DIR = "./chroma_db"
COLLECTION_NAME = "rag_documents"

# Models cho request/response
class QuestionRequest(BaseModel):
    question: str
    top_k: Optional[int] = 3

class QuestionResponse(BaseModel):
    answer: str
    references: List[str]
    source_documents: List[dict]

# ============= BÆ¯á»šC 1: LOAD DOCUMENT =============

def load_document(file_path: str, file_extension: str):
    """
    Load tÃ i liá»‡u tá»« file
    - PDF: dÃ¹ng PyPDFLoader
    - TXT: dÃ¹ng TextLoader  
    - DOCX: dÃ¹ng Docx2txtLoader
    """
    print(f"ğŸ“„ Äang load file...")
    
    if file_extension == ".pdf":
        loader = PyPDFLoader(file_path)
    elif file_extension == ".txt":
        loader = TextLoader(file_path, encoding='utf-8')
    elif file_extension in [".docx", ".doc"]:
        loader = Docx2txtLoader(file_path)
    else:
        raise ValueError(f"File type khÃ´ng Ä‘Æ°á»£c há»— trá»£: {file_extension}")
    
    documents = loader.load()
    print(f"âœ… Load xong {len(documents)} trang/Ä‘oáº¡n")
    return documents

# ============= BÆ¯á»šC 2: CHUNK DOCUMENT =============

def chunk_documents(documents):
    """
    Chia tÃ i liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunks)
    - Má»—i chunk: 1000 kÃ½ tá»±
    - Overlap: 200 kÃ½ tá»± (Ä‘á»ƒ giá»¯ context giá»¯a cÃ¡c chunk)
    """
    print(f"âœ‚ï¸ Äang chia nhá» tÃ i liá»‡u...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,        # Má»—i chunk 1000 kÃ½ tá»±
        chunk_overlap=200,      # Chá»“ng láº¥n 200 kÃ½ tá»±
        length_function=len,
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"âœ… Chia xong thÃ nh {len(chunks)} chunks")
    return chunks

# ============= BÆ¯á»šC 3: Táº O EMBEDDINGS =============

def create_embeddings():
    """Táº¡o embedding model tá»« Gemini"""
    print(f"ğŸ”§ Khá»Ÿi táº¡o Gemini Embedding model...")
    
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=GOOGLE_API_KEY
    )
    
    print(f"âœ… Embedding model sáºµn sÃ ng")
    return embeddings

def create_vector_store(chunks, embeddings):
    """
    Táº¡o vector store (database vector) tá»« chunks vá»›i ChromaDB
    - Má»—i chunk sáº½ Ä‘Æ°á»£c chuyá»ƒn thÃ nh vector (embedding)
    - LÆ°u vÃ o ChromaDB (persist trÃªn disk)
    """
    print(f"ğŸ—„ï¸ Äang táº¡o ChromaDB vector database...")
    
    # Táº¡o thÆ° má»¥c persist náº¿u chÆ°a cÃ³
    os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)
    
    # Táº¡o unique IDs cho má»—i chunk
    ids = [str(uuid.uuid4()) for _ in chunks]
    
    # Táº¡o ChromaDB
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=COLLECTION_NAME,
        persist_directory=CHROMA_PERSIST_DIR,
        ids=ids
    )
    
    print(f"âœ… ChromaDB Ä‘Ã£ sáºµn sÃ ng vá»›i {len(chunks)} vectors")
    return vector_store

def load_existing_vector_store(embeddings):
    """Load vector store Ä‘Ã£ tá»“n táº¡i tá»« disk"""
    print(f"ğŸ“‚ Äang load ChromaDB tá»« {CHROMA_PERSIST_DIR}...")
    
    vector_store = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_PERSIST_DIR
    )
    
    count = vector_store._collection.count()
    print(f"âœ… Load thÃ nh cÃ´ng {count} documents")
    return vector_store

# ============= BÆ¯á»šC 4: TÃŒM KIáº¾M RELEVANT DOCUMENTS =============

def search_relevant_docs(query: str, top_k: int = 3):
    """
    TÃ¬m kiáº¿m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan Ä‘áº¿n cÃ¢u há»i
    - Query Ä‘Æ°á»£c chuyá»ƒn thÃ nh vector
    - So sÃ¡nh vá»›i cÃ¡c vectors trong database
    - Tráº£ vá» top_k Ä‘oáº¡n vÄƒn giá»‘ng nháº¥t
    """
    global vector_store
    
    if vector_store is None:
        raise ValueError("Vector store chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
    
    print(f"ğŸ” Äang tÃ¬m kiáº¿m {top_k} Ä‘oáº¡n vÄƒn liÃªn quan...")
    
    # TÃ¬m kiáº¿m similarity
    docs = vector_store.similarity_search(query, k=top_k)
    
    print(f"âœ… TÃ¬m tháº¥y {len(docs)} Ä‘oáº¡n vÄƒn liÃªn quan")
    return docs

# ============= KHá»I Táº O LLM =============

def init_llm():
    """Khá»Ÿi táº¡o LLM vá»›i config tÃ¹y chá»‰nh"""
    print(f"ğŸ”§ Äang khá»Ÿi táº¡o LLM...")
    
    llm = ChatOpenAI(
        model="openai/gpt-oss-20b",
        base_url=LLM_BASE_URL,
        api_key="empty",
        stream_usage=True,
        reasoning_effort="low",
        temperature=0,
    )
    
    print(f"âœ… LLM Ä‘Ã£ sáºµn sÃ ng")
    return llm

# ============= BÆ¯á»šC 5: Táº O CONTEXT Vá»šI TRÃCH DáºªN =============

def build_context_with_references(context_docs: list):
    """
    Táº¡o context cÃ³ reference_id vÃ  danh sÃ¡ch tÃ i liá»‡u tham kháº£o
    Format giá»‘ng nhÆ° prompt máº«u
    """
    # Táº¡o text chunks vá»›i reference_id
    text_chunks = []
    reference_list = []
    
    for idx, doc in enumerate(context_docs, start=1):
        # Láº¥y metadata
        source = doc.metadata.get('source', 'Unknown')
        page = doc.metadata.get('page', 'N/A')
        
        # Táº¡o tÃªn file tá»« source path
        file_name = Path(source).name if source != 'Unknown' else 'Unknown'
        
        # Text chunk vá»›i reference_id
        chunk_info = {
            "reference_id": idx,
            "content": doc.page_content,
            "source": file_name,
            "page": page
        }
        text_chunks.append(chunk_info)
        
        # Reference list item
        if page != 'N/A':
            ref_item = f"[{idx}] {file_name} - Trang {page}"
        else:
            ref_item = f"[{idx}] {file_name}"
        reference_list.append(ref_item)
    
    # Format thÃ nh chuá»—i
    import json
    text_chunks_str = json.dumps(text_chunks, ensure_ascii=False, indent=2)
    reference_list_str = "\n".join(reference_list)
    
    # Build context theo template
    context = f"""CÃ¡c Ä‘oáº¡n vÄƒn báº£n (Má»—i má»¥c cÃ³ má»™t reference_id tÆ°Æ¡ng á»©ng vá»›i 'Danh sÃ¡ch tÃ i liá»‡u tham kháº£o'):

```json
{text_chunks_str}
```

Danh sÃ¡ch tÃ i liá»‡u tham kháº£o (Má»—i má»¥c báº¯t Ä‘áº§u báº±ng [reference_id] tÆ°Æ¡ng á»©ng vá»›i cÃ¡c má»¥c trong 'CÃ¡c Ä‘oáº¡n vÄƒn báº£n'):

```
{reference_list_str}
```"""
    
    return context, reference_list

# ============= BÆ¯á»šC 6: Gá»ŒI LLM Vá»šI PROMPT CÃ“ TRÃCH DáºªN =============

def generate_answer_with_citations(question: str, context_docs: list):
    """
    Gá»i LLM Ä‘á»ƒ generate cÃ¢u tráº£ lá»i CÃ“ TRÃCH DáºªN
    - Format context vá»›i reference_id
    - LLM sáº½ tráº£ lá»i kÃ¨m [1], [2], [3]... 
    - CÃ³ má»¥c "TÃ i liá»‡u tham kháº£o" á»Ÿ cuá»‘i
    """
    global llm
    
    print(f"ğŸ¤– Äang gá»i LLM Ä‘á»ƒ generate cÃ¢u tráº£ lá»i cÃ³ trÃ­ch dáº«n...")
    
    # Khá»Ÿi táº¡o LLM náº¿u chÆ°a cÃ³
    if llm is None:
        llm = init_llm()
    
    # Build context vá»›i references
    context, reference_list = build_context_with_references(context_docs)
    
    # Táº¡o prompt theo template
    system_prompt = """---Vai trÃ²---
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn gia, chuyÃªn tá»•ng há»£p thÃ´ng tin tá»« tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p. 
Báº¡n tráº£ lá»i cÃ¢u há»i CHá»ˆ dá»±a trÃªn thÃ´ng tin cÃ³ trong **Ngá»¯ cáº£nh**.

---HÆ°á»›ng dáº«n---
1. Tráº£ lá»i cÃ¢u há»i dá»±a HOÃ€N TOÃ€N vÃ o cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p
2. Sá»­ dá»¥ng trÃ­ch dáº«n [1], [2], [3]... khi Ä‘á» cáº­p Ä‘áº¿n thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n vÄƒn báº£n
3. Cuá»‘i cÃ¢u tráº£ lá»i, táº¡o má»¥c "### TÃ i liá»‡u tham kháº£o" liá»‡t kÃª cÃ¡c nguá»“n Ä‘Ã£ sá»­ dá»¥ng
4. Format: `- [n] TÃªn file - Trang X`
5. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, nÃ³i rÃµ "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u"
6. Sá»­ dá»¥ng Markdown Ä‘á»ƒ format cÃ¢u tráº£ lá»i rÃµ rÃ ng

---VÃ­ dá»¥ Ä‘á»‹nh dáº¡ng TÃ i liá»‡u tham kháº£o---
### TÃ i liá»‡u tham kháº£o

- [1] Quy cháº¿ ná»™i bá»™.pdf - Trang 5
- [2] HÆ°á»›ng dáº«n nhÃ¢n sá»±.docx - Trang 12
"""
    
    user_prompt = f"""---Ngá»¯ cáº£nh---
{context}

---CÃ¢u há»i---
{question}

---CÃ¢u tráº£ lá»i (cÃ³ trÃ­ch dáº«n)---"""
    
    # Gá»i LLM
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    response = llm.invoke(messages)
    answer = response.content
    
    print(f"âœ… ÄÃ£ nháº­n cÃ¢u tráº£ lá»i cÃ³ trÃ­ch dáº«n tá»« LLM")
    
    return answer, reference_list

# ============= API ENDPOINTS =============

@app.get("/")
async def root():
    """Kiá»ƒm tra API cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng"""
    has_existing_db = os.path.exists(CHROMA_PERSIST_DIR) and os.path.isdir(CHROMA_PERSIST_DIR)
    
    return {
        "message": "RAG System API Ä‘ang cháº¡y",
        "status": "active",
        "documents_loaded": vector_store is not None,
        "has_persisted_data": has_existing_db
    }

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    ENDPOINT 1: Upload file vÃ  xá»­ lÃ½
    
    Flow Ä‘Æ¡n giáº£n:
    1. LÆ°u file táº¡m thá»i
    2. Load document tá»« file
    3. Chia nhá» thÃ nh chunks
    4. Táº¡o embeddings cho má»—i chunk
    5. LÆ°u vÃ o ChromaDB (persist trÃªn disk)
    """
    global vector_store, embeddings
    
    try:
        print("\n" + "="*50)
        print("Báº®T Äáº¦U Xá»¬ LÃ FILE")
        print("="*50)
        
        # Kiá»ƒm tra loáº¡i file
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in [".pdf", ".txt", ".docx", ".doc"]:
            raise HTTPException(
                status_code=400,
                detail="Chá»‰ há»— trá»£ file PDF, TXT, DOCX"
            )
        
        # LÆ°u file táº¡m thá»i
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        # BÆ¯á»šC 1: Load document
        documents = load_document(tmp_file_path, file_extension)
        
        # FIX: Cáº­p nháº­t metadata vá»›i tÃªn file gá»‘c
        for doc in documents:
            doc.metadata['source'] = file.filename
            doc.metadata['original_filename'] = file.filename
        
        # BÆ¯á»šC 2: Chunk documents
        chunks = chunk_documents(documents)
        
        # BÆ¯á»šC 3: Táº¡o embeddings náº¿u chÆ°a cÃ³
        if embeddings is None:
            embeddings = create_embeddings()
        
        # BÆ¯á»šC 4: Táº¡o hoáº·c update ChromaDB
        if vector_store is None:
            # Táº¡o má»›i hoáº·c load existing
            if os.path.exists(CHROMA_PERSIST_DIR):
                print("ğŸ“‚ PhÃ¡t hiá»‡n ChromaDB Ä‘Ã£ tá»“n táº¡i, Ä‘ang load...")
                vector_store = load_existing_vector_store(embeddings)
                # Add thÃªm documents má»›i
                ids = [str(uuid.uuid4()) for _ in chunks]
                vector_store.add_documents(chunks, ids=ids)
                print(f"â• ÄÃ£ thÃªm {len(chunks)} chunks má»›i")
            else:
                vector_store = create_vector_store(chunks, embeddings)
        else:
            # Add documents vÃ o vector store hiá»‡n táº¡i
            ids = [str(uuid.uuid4()) for _ in chunks]
            vector_store.add_documents(chunks, ids=ids)
            print(f"â• ÄÃ£ thÃªm {len(chunks)} chunks má»›i")
        
        # XÃ³a file táº¡m
        os.unlink(tmp_file_path)
        
        print("="*50)
        print("HOÃ€N THÃ€NH Xá»¬ LÃ FILE")
        print("="*50 + "\n")
        
        # Äáº¿m tá»•ng sá»‘ documents
        total_docs = vector_store._collection.count()
        
        return JSONResponse(content={
            "message": "Upload vÃ  xá»­ lÃ½ file thÃ nh cÃ´ng!",
            "filename": file.filename,
            "chunks_added": len(chunks),
            "total_documents": total_docs,
            "status": "ready"
        })
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """
    ENDPOINT 2: Há»i Ä‘Ã¡p vá» tÃ i liá»‡u CÃ“ TRÃCH DáºªN
    
    Flow Ä‘Æ¡n giáº£n:
    1. Nháº­n cÃ¢u há»i tá»« user
    2. TÃ¬m kiáº¿m cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan trong vector store
    3. Build context vá»›i reference_id [1], [2], [3]...
    4. Gá»­i context + cÃ¢u há»i cho LLM
    5. LLM tráº£ lá»i kÃ¨m trÃ­ch dáº«n trong cÃ¢u tráº£ lá»i
    6. Tráº£ vá»: answer + references + source documents
    """
    global vector_store
    
    if vector_store is None:
        raise HTTPException(
            status_code=400,
            detail="ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c upload. Vui lÃ²ng upload file trÆ°á»›c!"
        )
    
    try:
        print("\n" + "="*50)
        print(f"CÃ‚U Há»I: {request.question}")
        print("="*50)
        
        # BÆ¯á»šC 1: TÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan
        relevant_docs = search_relevant_docs(request.question, request.top_k)
        
        # BÆ¯á»šC 2: Generate cÃ¢u tráº£ lá»i CÃ“ TRÃCH DáºªN
        answer, reference_list = generate_answer_with_citations(request.question, relevant_docs)
        
        # Format source documents chi tiáº¿t
        sources = []
        for i, doc in enumerate(relevant_docs, start=1):
            source_file = Path(doc.metadata.get('source', 'Unknown')).name
            page = doc.metadata.get('page', 'N/A')
            
            sources.append({
                "reference_id": i,
                "file_name": source_file,
                "page": page,
                "content_preview": doc.page_content[:300] + "...",
                "metadata": doc.metadata
            })
        
        print("="*50)
        print(f"TRáº¢ Lá»œI (cÃ³ trÃ­ch dáº«n): {answer[:150]}...")
        print(f"Sá» TÃ€I LIá»†U THAM KHáº¢O: {len(reference_list)}")
        print("="*50 + "\n")
        
        return QuestionResponse(
            answer=answer,
            references=reference_list,
            source_documents=sources
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/load")
async def load_persisted_db():
    """Load ChromaDB Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trÆ°á»›c Ä‘Ã³"""
    global vector_store, embeddings
    
    if not os.path.exists(CHROMA_PERSIST_DIR):
        raise HTTPException(
            status_code=404,
            detail="KhÃ´ng tÃ¬m tháº¥y ChromaDB Ä‘Ã£ lÆ°u. Vui lÃ²ng upload file trÆ°á»›c!"
        )
    
    try:
        # Táº¡o embeddings náº¿u chÆ°a cÃ³
        if embeddings is None:
            embeddings = create_embeddings()
        
        # Load vector store
        vector_store = load_existing_vector_store(embeddings)
        
        total_docs = vector_store._collection.count()
        
        return {
            "message": "Load ChromaDB thÃ nh cÃ´ng!",
            "total_documents": total_docs,
            "status": "ready"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.delete("/reset")
async def reset_system():
    """Reset toÃ n bá»™ há»‡ thá»‘ng vÃ  xÃ³a ChromaDB"""
    global vector_store, embeddings, llm
    
    vector_store = None
    embeddings = None
    llm = None
    
    # XÃ³a ChromaDB folder
    if os.path.exists(CHROMA_PERSIST_DIR):
        shutil.rmtree(CHROMA_PERSIST_DIR)
        print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a ChromaDB táº¡i {CHROMA_PERSIST_DIR}")
    
    return {"message": "ÄÃ£ reset há»‡ thá»‘ng vÃ  xÃ³a database thÃ nh cÃ´ng"}

@app.get("/status")
async def get_status():
    """Kiá»ƒm tra tráº¡ng thÃ¡i há»‡ thá»‘ng"""
    has_persisted = os.path.exists(CHROMA_PERSIST_DIR)
    total_docs = 0
    
    if vector_store is not None:
        try:
            total_docs = vector_store._collection.count()
        except:
            pass
    
    return {
        "vector_store_loaded": vector_store is not None,
        "has_persisted_data": has_persisted,
        "total_documents": total_docs,
        "system_ready": vector_store is not None,
        "message": "Sáºµn sÃ ng" if vector_store is not None else "ChÆ°a upload tÃ i liá»‡u"
    }

# ============= MAIN =============

if __name__ == "__main__":
    import uvicorn
    
    print("""
    ========================================
    ğŸš€ RAG SYSTEM API - ChromaDB Version
    ========================================
    
    ğŸ“Œ API Endpoints:
    
    1. POST /upload
       â†’ Upload file (PDF/TXT/DOCX)
       â†’ Tá»± Ä‘á»™ng chunk vÃ  embedding
       â†’ LÆ°u vÃ o ChromaDB (persist)
    
    2. POST /ask
       â†’ Há»i Ä‘Ã¡p vá» tÃ i liá»‡u (cÃ³ trÃ­ch dáº«n)
       â†’ Body: {"question": "cÃ¢u há»i cá»§a báº¡n"}
    
    3. POST /load
       â†’ Load ChromaDB Ä‘Ã£ lÆ°u tá»« láº§n trÆ°á»›c
    
    4. GET /status
       â†’ Kiá»ƒm tra há»‡ thá»‘ng & sá»‘ documents
    
    5. DELETE /reset
       â†’ XÃ³a database vÃ  reset há»‡ thá»‘ng
    
    ========================================
    ğŸŒ Server Ä‘ang cháº¡y táº¡i: http://localhost:8080
    ğŸ“– API docs: http://localhost:8080/docs
    ========================================
    """)
    
    uvicorn.run(app, host="0.0.0.0", port=8080)